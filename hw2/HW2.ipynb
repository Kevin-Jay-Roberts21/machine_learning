{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ2CFjAsTh4L"
   },
   "source": [
    "# Homework 2: Logistic Regression with L2 regularization\n",
    "\n",
    "**Please type your name and A number here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DQeOFPFEVknu"
   },
   "outputs": [],
   "source": [
    "Name = \"Kevin Roberts\"\n",
    "assert Name != \"\", 'Please enter your name in the above quotation marks, thanks!'\n",
    "\n",
    "\n",
    "A_number = \"A02256264\"\n",
    "assert A_number != \"\", 'Please enter your A-number in the above quotation marks, thanks!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9A464a2Vknw"
   },
   "source": [
    "The goal of this second notebook is to implement your own logistic regression classifier with L2 regularization. You will do the following:\n",
    "\n",
    " * Extract features from Amazon product reviews.\n",
    " * Convert a DataFrame into a NumPy array.\n",
    " * Write a function to compute the derivative of log likelihood function with an L2 penalty with respect to a single coefficient.\n",
    " * Implement gradient descent with an L2 penalty.\n",
    " * Empirically explore how the L2 penalty can ameliorate overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksz2byQ7Th4Q"
   },
   "source": [
    "## Load and process review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZThaM54dTh4Q"
   },
   "source": [
    "For this assignment, we will use a subset of the Amazon product review dataset. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted of mostly positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fGLgIL_bTh4Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "products = pd.read_csv('amazon_baby_subset.csv')\n",
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmwQEADaTh4R"
   },
   "source": [
    "We will work with a hand-curated list of important words extracted from the review data. We will also perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in string](https://docs.python.org/3/library/string.html) functionality.\n",
    "2. Compute word counts (only for the **important_words**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IoV7ftZgTh4R"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Remove punctuation.\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "\n",
    "# Split out the words into individual columns\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y10WjhXsTh4S"
   },
   "source": [
    "Now, let us take a look at what the dataset looks like (**Note:** This may take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n5CDREMzTh4S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>All of my kids have cried nonstop when I tried...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter had her 1st baby over a year ago S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>One of babys first and favorite books and it i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Very cute interactive book My son loves this b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>Samsung Baby Care Washer, Stainless Platinum, ...</td>\n",
       "      <td>My infant goes to a really crappy daycare, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>My infant goes to a really crappy daycare and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>Mud Pie Milestone Stickers, Boy</td>\n",
       "      <td>Pretty please open and inspect these stickers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Pretty please open and inspect these stickers ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>Best BIB for Baby - Soft Bib (Pink-Elephant)</td>\n",
       "      <td>Great 5-Star Product but An Obvious knock-off ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Great 5Star Product but An Obvious knockoff of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>Bouncy&amp;reg; Inflatable Real Feel Hopping Cow</td>\n",
       "      <td>When I received the item my initial thought wa...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>When I received the item my initial thought wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>Maxboost iPhone 5S/5 Case - Protective Snap-on...</td>\n",
       "      <td>I got this case in the mail today, it came on ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>I got this case in the mail today it came on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53072 rows Ã— 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      Stop Pacifier Sucking without tears with Thumb...   \n",
       "1        Nature's Lullabies Second Year Sticker Calendar   \n",
       "2        Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                            Lamaze Peekaboo, I Love You   \n",
       "4      SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "...                                                  ...   \n",
       "53067  Samsung Baby Care Washer, Stainless Platinum, ...   \n",
       "53068                    Mud Pie Milestone Stickers, Boy   \n",
       "53069       Best BIB for Baby - Soft Bib (Pink-Elephant)   \n",
       "53070       Bouncy&reg; Inflatable Real Feel Hopping Cow   \n",
       "53071  Maxboost iPhone 5S/5 Case - Protective Snap-on...   \n",
       "\n",
       "                                                  review  rating  sentiment  \\\n",
       "0      All of my kids have cried non-stop when I trie...       5          1   \n",
       "1      We wanted to get something to keep track of ou...       5          1   \n",
       "2      My daughter had her 1st baby over a year ago. ...       5          1   \n",
       "3      One of baby's first and favorite books, and it...       4          1   \n",
       "4      Very cute interactive book! My son loves this ...       5          1   \n",
       "...                                                  ...     ...        ...   \n",
       "53067  My infant goes to a really crappy daycare, and...       1         -1   \n",
       "53068  Pretty please open and inspect these stickers ...       1         -1   \n",
       "53069  Great 5-Star Product but An Obvious knock-off ...       1         -1   \n",
       "53070  When I received the item my initial thought wa...       2         -1   \n",
       "53071  I got this case in the mail today, it came on ...       2         -1   \n",
       "\n",
       "                                            review_clean  baby  one  great  \\\n",
       "0      All of my kids have cried nonstop when I tried...     0    0      1   \n",
       "1      We wanted to get something to keep track of ou...     0    0      0   \n",
       "2      My daughter had her 1st baby over a year ago S...     1    0      0   \n",
       "3      One of babys first and favorite books and it i...     0    0      0   \n",
       "4      Very cute interactive book My son loves this b...     0    0      1   \n",
       "...                                                  ...   ...  ...    ...   \n",
       "53067  My infant goes to a really crappy daycare and ...     1    0      0   \n",
       "53068  Pretty please open and inspect these stickers ...     0    0      0   \n",
       "53069  Great 5Star Product but An Obvious knockoff of...     0    0      0   \n",
       "53070  When I received the item my initial thought wa...     0    1      0   \n",
       "53071  I got this case in the mail today it came on t...     0    0      0   \n",
       "\n",
       "       love  use  ...  seems  picture  completely  wish  buying  babies  won  \\\n",
       "0         0    0  ...      0        0           0     0       0       0    0   \n",
       "1         0    0  ...      0        0           0     0       0       0    0   \n",
       "2         0    0  ...      0        0           0     0       0       0    0   \n",
       "3         0    0  ...      0        0           0     0       0       0    0   \n",
       "4         0    0  ...      0        0           0     0       0       1    0   \n",
       "...     ...  ...  ...    ...      ...         ...   ...     ...     ...  ...   \n",
       "53067     0    0  ...      0        0           0     0       0       0    0   \n",
       "53068     0    0  ...      0        0           0     0       0       0    0   \n",
       "53069     0    0  ...      0        0           0     0       0       0    0   \n",
       "53070     0    0  ...      0        0           0     0       0       0    0   \n",
       "53071     0    0  ...      0        2           1     0       0       0    0   \n",
       "\n",
       "       tub  almost  either  \n",
       "0        0       0       0  \n",
       "1        0       0       0  \n",
       "2        0       0       0  \n",
       "3        0       0       0  \n",
       "4        0       0       0  \n",
       "...    ...     ...     ...  \n",
       "53067    0       0       0  \n",
       "53068    0       0       0  \n",
       "53069    0       0       0  \n",
       "53070    0       0       0  \n",
       "53071    0       0       0  \n",
       "\n",
       "[53072 rows x 198 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYwS35mbTh4S"
   },
   "source": [
    "## Train-Validation split\n",
    "\n",
    "We split the data into a train-validation split with 80% of the data in the training set and 20% of the data in the validation set. We use the `seed 42` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EEA82H34dn5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   : 42457 data points\n",
      "Validation set : 10615 data points\n"
     ]
    }
   ],
   "source": [
    "products_shuffled = products.sample(frac=1, ignore_index=True, random_state=42)\n",
    "train_index = int(0.8*len(products_shuffled))\n",
    "train_data = products_shuffled.iloc[:train_index]\n",
    "validation_data = products_shuffled.iloc[train_index:]\n",
    "print('Training set   : %d data points' % len(train_data))\n",
    "print('Validation set : %d data points' % len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL2xx3dUTh4T"
   },
   "source": [
    "## Convert DataFrame to NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zbZCydc_Th4T"
   },
   "source": [
    "We provide you with a function that extracts columns from a DataFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels.\n",
    "\n",
    "**Note:** The feature matrix includes an additional column 'intercept' filled with 1's to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6vr_oLbrTh4T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_numpy_data(df, features, label):\n",
    "    df['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_df = df[features]\n",
    "    feature_matrix = features_df.to_numpy()\n",
    "    label_sarray = df[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l1noUYDTh4T"
   },
   "source": [
    "We convert both the training and validation sets into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wcUTTuwMTh4T"
   },
   "outputs": [],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y4JIC5fTh4U"
   },
   "source": [
    "## Building on logistic regression with no L2 penalty assignment\n",
    "\n",
    "Recall from lecture that the function for logistic regression can be defined as:\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ is given by the word counts of **important_words** in the review $\\mathbf{x}_i$.\n",
    "\n",
    "<font color='red'> **rubric={15 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "InV_1mC2Th4U"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients\n",
    "    w_dot_h = np.dot(feature_matrix, coefficients)\n",
    "\n",
    "    # Compute P(y_i = +1 | x_i, w)\n",
    "    predictions = 1/(1 + np.exp(-w_dot_h))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNqeEY92Th4U"
   },
   "source": [
    "# Adding  L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU6EXV3rTh4U"
   },
   "source": [
    "Let us now work on extending logistic regression with L2 regularization. As discussed in the lectures, the L2 regularization is particularly useful in preventing overfitting. In this assignment, we will explore L2 regularization in detail.\n",
    "\n",
    "Recall from the lecture that for logistic regression without an L2 penalty, the derivative of the log likelihood function with respect to the $j$-th feature of $\\mathbf{x}_i$, denoted as $h_j(\\mathbf{x}_i)$, is:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})-\\mathbf{1}[y_i = +1]\\right)\n",
    "$$\n",
    "\n",
    "**Adding L2 penalty to the derivative**\n",
    "\n",
    "It takes only a small modification to add a L2 penalty. All terms indicated in **red** refer to terms that were added due to an **L2 penalty**.\n",
    "\n",
    "* Recall from the lecture that the activation function is still the sigmoid:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "* We add the L2 penalty term to the per-coefficient derivative of log likelihood:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})-\\mathbf{1}[y_i = +1]\\right) \\color{red}{+2\\lambda w_j }\n",
    "$$\n",
    "\n",
    "The **per-coefficient derivative for logistic regression with an L2 penalty** is as follows:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})-\\mathbf{1}[y_i = +1]\\right) \\color{red}{+2\\lambda w_j }\n",
    "$$\n",
    "and for the intercept term, we have\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_0} = \\sum_{i=1}^N h_0(\\mathbf{x}_i)\\left(P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})-\\mathbf{1}[y_i = +1]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6WltXrFTh4U"
   },
   "source": [
    "**Note**: As we did in the Regression course, we do not apply the L2 penalty on the intercept. A large intercept does not necessarily indicate overfitting because the intercept is not associated with any particular feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdVAsKtLTh4V"
   },
   "source": [
    "Write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. Unlike its counterpart in the last assignment, the function accepts five arguments:\n",
    " * `errors` vector containing $(P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})-\\mathbf{1}[y_i = +1])$ for all $i$\n",
    " * `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$\n",
    " * `coefficient` containing the current value of coefficient $w_j$.\n",
    " * `l2_penalty` representing the L2 penalty constant $\\lambda$\n",
    " * `feature_is_constant` telling whether the $j$-th feature is constant or not.\n",
    "\n",
    " <font color='red'> **rubric={15 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HnNI0bJlTh4V"
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant):\n",
    "\n",
    "    # Compute the dot product of errors and feature\n",
    "    errors_dot_feature = np.dot(errors, feature)\n",
    "    \n",
    "    derivative = errors_dot_feature\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant:\n",
    "        derivative = errors_dot_feature + 2*l2_penalty*coefficient\n",
    "\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62QeqQ-bTh4V"
   },
   "source": [
    "<font color='red'> **rubric={5 points}** </font> \n",
    "\n",
    "**Quiz Question:** In the code above, was the intercept term regularized?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XruM0JGVkn2"
   },
   "source": [
    "**Answer:** No, because of the 'if not feature_is_constant' if statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KVlftepTh4V"
   },
   "source": [
    "To verify the correctness of the gradient descent algorithm, we provide a function for computing negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi5SToioTh4V"
   },
   "source": [
    "$$\\ell\\ell(\\mathbf{w}) = - \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) \\color{red}{+ \\lambda\\|\\mathbf{w}\\|_2^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "88ufSrQGTh4V"
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "\n",
    "    lp = -np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) + l2_penalty*np.sum(coefficients[1:]**2)\n",
    "\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxnLWrRXTh4W"
   },
   "source": [
    "**Quiz Question:** Does the term with L2 regularization increase or decrease $\\ell\\ell(\\mathbf{w})$?  \n",
    "\n",
    " <font color='red'> **rubric={5 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ZfVhDUVkn2"
   },
   "source": [
    "**Answer:** The term decreases ll(w) since it adds a penalty to the negative log likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkH0ZFTDTh4W"
   },
   "source": [
    "Fill in the code below to complete the logistic regression function with L2 penalty.\n",
    "\n",
    "<font color='red'> **rubric={30 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LWVUjKhuTh4W"
   },
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "\n",
    "        # Compute the errors as predictions - indicator\n",
    "        errors = predictions - indicator\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = np.dot(feature_matrix[:,j], errors)\n",
    "\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] -= step_size * derivative\n",
    "\n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGWijcIuTh4W"
   },
   "source": [
    "# Explore effects of L2 regularization\n",
    "\n",
    "Now that we have written up all the pieces needed for regularized logistic regression, let's explore the benefits of using **L2 regularization** in analyzing sentiment for product reviews. **As iterations pass, the log likelihood should increase**.\n",
    "\n",
    "Below, we train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lRxF2HqBTh4W",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = 29246.01157659\n",
      "iteration   1: log likelihood of observed labels = 29070.58360222\n",
      "iteration   2: log likelihood of observed labels = 28901.88872854\n",
      "iteration   3: log likelihood of observed labels = 28739.35473391\n",
      "iteration   4: log likelihood of observed labels = 28582.53738048\n",
      "iteration   5: log likelihood of observed labels = 28431.07439712\n",
      "iteration   6: log likelihood of observed labels = 28284.65788683\n",
      "iteration   7: log likelihood of observed labels = 28143.01762536\n",
      "iteration   8: log likelihood of observed labels = 28005.91080612\n",
      "iteration   9: log likelihood of observed labels = 27873.11562229\n",
      "iteration  10: log likelihood of observed labels = 27744.42714907\n",
      "iteration  11: log likelihood of observed labels = 27619.65461465\n",
      "iteration  12: log likelihood of observed labels = 27498.61951642\n",
      "iteration  13: log likelihood of observed labels = 27381.15425730\n",
      "iteration  14: log likelihood of observed labels = 27267.10110720\n",
      "iteration  15: log likelihood of observed labels = 27156.31137271\n",
      "iteration  20: log likelihood of observed labels = 26646.66207864\n",
      "iteration  30: log likelihood of observed labels = 25806.30948611\n",
      "iteration  40: log likelihood of observed labels = 25140.35176932\n",
      "iteration  50: log likelihood of observed labels = 24597.88429775\n",
      "iteration  60: log likelihood of observed labels = 24146.15574971\n",
      "iteration  70: log likelihood of observed labels = 23763.18945871\n",
      "iteration  80: log likelihood of observed labels = 23433.69281804\n",
      "iteration  90: log likelihood of observed labels = 23146.68643154\n",
      "iteration 100: log likelihood of observed labels = 22894.07281406\n",
      "iteration 200: log likelihood of observed labels = 21389.39969140\n",
      "iteration 300: log likelihood of observed labels = 20677.83465055\n",
      "iteration 400: log likelihood of observed labels = 20258.46819793\n",
      "iteration 500: log likelihood of observed labels = 19982.08348942\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 0\n",
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=0, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iflppw7vTh4W"
   },
   "outputs": [],
   "source": [
    "# run with L2 = 4\n",
    "coefficients_4_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=4, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ5jTgb-Th4W"
   },
   "outputs": [],
   "source": [
    "# run with L2 = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=10, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95Me6c4iTh4W"
   },
   "outputs": [],
   "source": [
    "# run with L2 = 1e2\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e2, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwtu0SPMTh4X"
   },
   "outputs": [],
   "source": [
    "# run with L2 = 1e3\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e3, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPF53ezRTh4X"
   },
   "outputs": [],
   "source": [
    "# run with L2 = 1e5\n",
    "coefficients_1e5_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e5, max_iter=501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfBf99RSTh4X"
   },
   "source": [
    "## Compare coefficients\n",
    "\n",
    "We now compare the **coefficients** for each of the models that were trained above. We will create a DataFrame of features and learned coefficients associated with each of the different L2 penalty values.\n",
    "\n",
    "Below is a simple helper function that will help us create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWfIdKxpDePd"
   },
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame({'word': ['(intercept)'] + important_words})\n",
    "df_coef['coefficients [L2=0]'] = coefficients_0_penalty\n",
    "df_coef['coefficients [L2=4]'] = coefficients_4_penalty\n",
    "df_coef['coefficients [L2=10]'] = coefficients_10_penalty\n",
    "df_coef['coefficients [L2=1e2]'] = coefficients_1e2_penalty\n",
    "df_coef['coefficients [L2=1e3]'] = coefficients_1e3_penalty\n",
    "df_coef['coefficients [L2=1e5]'] = coefficients_1e5_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqL0h-FnTh4Y"
   },
   "source": [
    "Using **the coefficients trained with L2 penalty 0**, find the 5 most positive words (with largest positive coefficients). Save them to **positive_words**. Similarly, find the 5 most negative words (with largest negative coefficients) and save them to **negative_words**.\n",
    "\n",
    "<font color='red'> **rubric={15 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkHLBwv9Th4Y"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftd2vMA4Th4Y"
   },
   "source": [
    "Let us observe the effect of increasing L2 penalty on the 10 words just selected. We provide you with a utility function to  plot the coefficient path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSo-AEdjTh4Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "def make_coefficient_plot(df, positive_words, negative_words, l2_penalty_list):\n",
    "    cmap_positive = plt.get_cmap('Reds')\n",
    "    cmap_negative = plt.get_cmap('Blues')\n",
    "\n",
    "    xx = l2_penalty_list\n",
    "    plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "\n",
    "    lst_positive_words = df.loc[df['word'].isin(positive_words)].iloc[:,1:].values\n",
    "    lst_negative_words = df.loc[df['word'].isin(negative_words)].iloc[:,1:].values\n",
    "\n",
    "    for i in range(len(positive_words)):\n",
    "        color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "        plt.plot(xx, lst_positive_words[i:i+1].flatten(),\n",
    "                 '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "\n",
    "    for i in range(len(negative_words)):\n",
    "        color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "        plt.plot(xx, lst_negative_words[i:i+1].flatten(),\n",
    "                 '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "\n",
    "    plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "    plt.axis([1, 1e5, -1, 2])\n",
    "    plt.title('Coefficient path')\n",
    "    plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "    plt.ylabel('Coefficient value')\n",
    "    plt.xscale('log')\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CfqtsZUTh4Z"
   },
   "source": [
    "Run the following cell to generate the plot. Use the plot to answer the following quiz question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax4FWtVpTh4Z"
   },
   "outputs": [],
   "source": [
    "make_coefficient_plot(df_coef, positive_words, negative_words, l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tjbLsM-Th4Z"
   },
   "source": [
    "**Quiz Question**: (True/False) All coefficients consistently get smaller in size as the L2 penalty is increased.\n",
    "\n",
    "<font color='red'> **rubric={3 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy9cjX6RVkn-"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb0KQkTkVkn-"
   },
   "source": [
    "**Quiz Question**: (True/False) The relative order of coefficients is preserved as the L2 penalty is increased. (For example, if the coefficient for 'cat' was more positive than that for 'dog', this remains true as the L2 penalty increases.)\n",
    "\n",
    "<font color='red'> **rubric={3 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVAkDxxxVkn-"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ2CQU-NTh4Z"
   },
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "Now, let us compute the accuracy of the classifier model. Recall that the accuracy is given by\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{\\text{\\# correctly classified data points}}{\\text{\\# total data points}}\n",
    "$$\n",
    "\n",
    "\n",
    "Recall from lecture that that the class prediction is calculated using\n",
    "$$\n",
    "\\hat{y}_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & h(\\mathbf{x}_i)^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & h(\\mathbf{x}_i)^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "We can implement the code to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "m8xXCdEVTh4a"
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(feature_matrix, sentiment, coefficients):\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    apply_threshold = np.vectorize(lambda x: 1. if x > 0  else -1.)\n",
    "    predictions = apply_threshold(scores)\n",
    "\n",
    "    num_correct = (predictions == sentiment).sum()\n",
    "    accuracy = num_correct / len(feature_matrix)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCA_QY2STh4a"
   },
   "source": [
    "Below, we compare the accuracy on the **training data** and **validation data** for all the models that were trained in this assignment.  We first calculate the accuracy values and then build a simple report summarizing the performance for the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8dKUS2dTh4a"
   },
   "outputs": [],
   "source": [
    "train_accuracy = {}\n",
    "train_accuracy[0]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_0_penalty)\n",
    "train_accuracy[4]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_4_penalty)\n",
    "train_accuracy[10]  = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_10_penalty)\n",
    "train_accuracy[1e2] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e2_penalty)\n",
    "train_accuracy[1e3] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e3_penalty)\n",
    "train_accuracy[1e5] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e5_penalty)\n",
    "\n",
    "validation_accuracy = {}\n",
    "validation_accuracy[0]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_0_penalty)\n",
    "validation_accuracy[4]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_4_penalty)\n",
    "validation_accuracy[10]  = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_10_penalty)\n",
    "validation_accuracy[1e2] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e2_penalty)\n",
    "validation_accuracy[1e3] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e3_penalty)\n",
    "validation_accuracy[1e5] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e5_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmaYe4BnTh4a"
   },
   "outputs": [],
   "source": [
    "# Build a simple report\n",
    "for key in sorted(validation_accuracy.keys()):\n",
    "    print(\"L2 penalty = %g\" % key)\n",
    "    print(\"train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[key], validation_accuracy[key]))\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1wqzPv1tTh4b"
   },
   "outputs": [],
   "source": [
    "# Optional. Plot accuracy on training and validation sets over choice of L2 penalty.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "sorted_list = sorted(train_accuracy.items(), key=lambda x:x[0])\n",
    "plt.plot([p[0] for p in sorted_list], [p[1] for p in sorted_list], 'bo-', linewidth=4, label='Training accuracy')\n",
    "sorted_list = sorted(validation_accuracy.items(), key=lambda x:x[0])\n",
    "plt.plot([p[0] for p in sorted_list], [p[1] for p in sorted_list], 'ro-', linewidth=4, label='Validation accuracy')\n",
    "plt.xscale('symlog')\n",
    "plt.axis([0, 1e5, 0.66, 0.786])\n",
    "plt.legend(loc='lower left')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GGhDcAdTh4b"
   },
   "source": [
    "**Quiz Question**: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the **highest** accuracy on the **training** data?\n",
    "\n",
    "<font color='red'> **rubric={3 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJqPfBSdVkoA"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKpNOOufVkoB"
   },
   "source": [
    "**Quiz Question**: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the **highest** accuracy on the **validation** data?\n",
    "\n",
    "<font color='red'> **rubric={3 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xw2KtLcVkoB"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y34tkQ2kVkoB"
   },
   "source": [
    "**Quiz Question**: Does the **highest** accuracy on the **training** data imply that the model is the best one?\n",
    "\n",
    "<font color='red'> **rubric={3 points}** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqonvbz4VkoB"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "64YCYjQCTh4b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
