{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxwipZiRq2c"
   },
   "source": [
    "# Convolutional Neural Networks: Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6VOixFaRc4N"
   },
   "source": [
    "**Please type your name and A number here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3asnhIURmbz"
   },
   "outputs": [],
   "source": [
    "Name = \"Kevin Roberts\"\n",
    "assert Name != \"\", 'Please enter your name in the above quotation marks, thanks!'\n",
    "\n",
    "A_number = \"A02256264\"\n",
    "assert A_number != \"\", 'Please enter your A-number in the above quotation marks, thanks!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUT7Gsz3RU4x"
   },
   "source": [
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Create a mood classifer using the Torch Sequential API\n",
    "- Build a ConvNet to identify sign language digits using the Torch Module API\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in PyTorch for a __binary__ classification problem\n",
    "- Build and train a ConvNet in PyTorch for a __multiclass__ classification problem\n",
    "- Explain different use cases for the Sequential and Module APIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT5aNKeKRU4z"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Packages](#1)\n",
    "    - [1.1 - Load the Data and Split the Data into Train/Test Sets](#1-1)\n",
    "- [2 - Layers in PyTorch](#2)\n",
    "- [3 - The Sequential API](#3)\n",
    "    - [3.1 - Create the Sequential Model](#3-1)\n",
    "        - [Exercise 1 - happyModel](#ex-1)\n",
    "    - [3.2 - Train and Evaluate the Model](#3-2)\n",
    "- [4 - The Module API](#4)\n",
    "    - [4.1 - Load the SIGNS Dataset](#4-1)\n",
    "    - [4.2 - Split the Data into Train/Test Sets](#4-2)\n",
    "    - [4.3 - Forward Propagation](#4-3)\n",
    "        - [Exercise 2 - convolutional_model](#ex-2)\n",
    "    - [4.4 - Train the Model](#4-4)\n",
    "- [5 - History Object](#5)\n",
    "- [6 - Bibliography](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X28sR-EcRU4z"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "As usual, begin by loading in the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t4Tr89OkRU4z"
   },
   "outputs": [],
   "source": [
    "### Modified 2/11/2025 by Nathan Nelson for Torch support in place of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvZ9ebfebCZ8",
    "outputId": "15c57423-66b0-430a-ac3b-eb1e921e3469"
   },
   "outputs": [],
   "source": [
    "### If you use Google Colab, you can install the torchinfo package by running the following command:\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLJxLvslRU40"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah7WgPnkRU40"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Load the Data and Split the Data into Train/Test Sets\n",
    "\n",
    "You'll be using the Happy House dataset for this part of the assignment, which contains images of peoples' faces. Your task will be to build a ConvNet that determines whether the people in the images are smiling or not -- because they only get to enter the house if they're smiling!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSy0h8HrRU40"
   },
   "outputs": [],
   "source": [
    "def load_happy_dataset():     # No need to modify unless using a different directory.\n",
    "    train_dataset = h5py.File('datasets/train_happy.h5', \"r\")\n",
    "    # your train set features\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(\n",
    "        train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_happy.h5', \"r\")\n",
    "    # your test set features\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(\n",
    "        test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "def load_signs_dataset():   # No need to modify unless using a different directory.\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    # your train set features\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(\n",
    "        train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    # your test set features\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(\n",
    "        test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SsTSqpNsRU40",
    "outputId": "e0a90be8-4ec0-437c-c6be-8f99bc4105af"
   },
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert data loading and preprocessing to PyTorch. Conv2D expects the inputs to be in the form ([Number of samples, Channels, Width, Height]), while the data are currently in the form ([Number of samples, Width, Height, Channels]). We can use the permute() function here to quickly change the data to the expected format.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # Convert to (N, C, H, W)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "Y_train = torch.tensor(Y_train_orig, dtype=torch.float32).T\n",
    "Y_test = torch.tensor(Y_test_orig, dtype=torch.float32).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UJGMtzZRU41"
   },
   "source": [
    "You can display the images contained in the dataset. Images are **64x64** pixels in RGB format (3 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "jsAGZwFLRU41",
    "outputId": "a6bae944-a247-47e8-c36c-bba7e60db218"
   },
   "outputs": [],
   "source": [
    "index = 124\n",
    "plt.imshow(X_train_orig[index]) #display sample training image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X817jrOBRU41"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Layers in PyTorch\n",
    "\n",
    "In Torch, you don't have to write code directly to create layers. Rather, Torch has pre-defined layers you can use.\n",
    "\n",
    "When you create a layer in Torch, you are creating a function that takes some input and transforms it into an output you can reuse later. Nice and easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2KnnVnbRU41"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - The Sequential API\n",
    "\n",
    "Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call.\n",
    "\n",
    "For the first part of this assignment, you'll create a model using Torch's Sequential API, which allows you to build layer by layer, and is ideal for building models where each layer has **exactly one** input tensor and **one** output tensor.\n",
    "\n",
    "As you'll see, using the Sequential API is simple and straightforward, but is only appropriate for simpler, more straightforward tasks. Later in this notebook you'll spend some time building with a more flexible, powerful alternative: the Module API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1hA7h9xRU42"
   },
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Create the Sequential Model\n",
    "\n",
    "As mentioned earlier, the PyTorch Sequential API can be used to build simple models with layer operations that proceed in a sequential order.\n",
    "\n",
    "You can think of a Sequential model as behaving like a list of layers. Like Python lists, Sequential layers are ordered, and the order in which they are specified matters.  If your model is non-linear or contains layers with multiple inputs or outputs, a Sequential model wouldn't be the right choice!\n",
    "\n",
    "For any layer construction in Keras, you'll need to specify the input shape in advance. This is because in Keras, the shape of the weights is based on the shape of the inputs. The weights are only created when the model first sees some input data. Sequential models can be created by passing a list of layers to the Sequential constructor, like you will do in the next assignment.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - happyModel\n",
    "\n",
    "Implement the `happyModel` function below to build the following model: `ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> LINEAR`. Take help from [torch.nn](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [nn.ZeroPad2d](https://pytorch.org/docs/stable/generated/torch.nn.ZeroPad2d.html): padding 3 x 3, input shape 3 x 64 x 64\n",
    " - [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): Use 32 out channels with 7x7 filters, stride 1 x 1, from 3 in channels\n",
    " - [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html): on 32 features\n",
    " - [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    " - [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html): Using kernel size of 2 x 2\n",
    " - [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) the previous output.\n",
    " - Fully-connected ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) layer: Apply a fully connected layer with 1 output neuron and then a sigmoid activation:\n",
    " - [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)\n",
    "\n",
    "\n",
    "<font color='red'> **rubric={30 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "4jtLn3SPRU42",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d28b191f257bdd5b70c7b8952559d5",
     "grade": false,
     "grade_id": "cell-0e56d3fc28b69aec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def happyModel():\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the binary classification model:\n",
    "    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL2D -> FLATTEN -> LINEAR -> SIGMOID\n",
    "\n",
    "    Note that for simplicity and grading purposes, you'll hard-code all the values\n",
    "    such as the stride and kernel (filter) sizes.\n",
    "    Normally, functions should take these values as function parameters.\n",
    "\n",
    "    Arguments:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    model -- PyTorch Sequential model\n",
    "    \"\"\"\n",
    "    #         # YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "    #         # YOUR CODE ENDS HERE\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRSrTM4TaC-2"
   },
   "outputs": [],
   "source": [
    "happy_model = happyModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sgp-wyjamqL",
    "outputId": "d7b961ea-64b6-45a4-8379-b5069bcf9502"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "summary(happy_model, input_size=(batch_size, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXNb4PouRU42"
   },
   "source": [
    "Now that your model is created, you can prepare it for training with an optimizer and loss of your choice\n",
    "\n",
    "<font color='red'> **rubric={5 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBb0vb6_RU42"
   },
   "outputs": [],
   "source": [
    "criterion =   # YOUR CODE HERE; Choose appropriate loss function\n",
    "optimizer = optim.Adam(happy_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKRH_LQgRU43"
   },
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Train and Evaluate the Model\n",
    "\n",
    "After creating the model, we will use DataLoaders to make batch training easier. Use the TensorDataset function followed by the DataLoader function to turn the training data into prepared batches (maybe try a batch size of 16 to start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTnRWi-6RU43"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyRikJdkRU43"
   },
   "source": [
    "We can now train the model! Make sure to call zero_grad() in every iteration.\n",
    "\n",
    "<font color='red'> **rubric={15 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSBEGMzqRU43",
    "outputId": "73246c48-a4d9-4154-9b46-5ca5865a8760"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    happy_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # YOUR CODE STARTS HERE (Hint: Forward pass, loss computation, backward pass, optimizer step) You may need to update the variable names in the following starter code here based on your implementation.\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Convert outputs to binary predictions\n",
    "        predicted = (outputs > 0.5).float()  # Threshold at 0.5\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9QwCC-ORU44"
   },
   "source": [
    "Now that we have trained on part of the dataset, let's test the model on the remaining part to know how it performs on unseen data. Be sure to set the model into eval mode. You can use the same loss function for testing as you did for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xC2FpG6mRU44",
    "outputId": "b8c78d4b-59a3-4dcc-a2cf-7056dd88e8e6"
   },
   "outputs": [],
   "source": [
    "happy_model.eval()\n",
    "\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "total_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = happy_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert outputs to binary predictions (0 or 1)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdlD6Bl0RU44"
   },
   "source": [
    "The test accuracy should be above 80%\n",
    "\n",
    "Easy, right? But what if you need to build a model with shared layers, branches, or multiple inputs and outputs? This is where Sequential, with its beautifully simple yet limited functionality, won't be able to help you.\n",
    "\n",
    "Next up: Enter the Module API, your slightly more complex, highly flexible friend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9jRp174RU44"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - The Module API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LRXGmAhRU44"
   },
   "source": [
    "Welcome to the second half of the assignment, where you'll use Torch's flexible [Module API](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) to build a ConvNet that can differentiate between 6 sign language digits.\n",
    "\n",
    "The Module API can handle models with non-linear topology, shared layers, as well as layers with multiple inputs or outputs. Imagine that, where the Sequential API requires the model to move in a linear fashion through its layers, the Module API allows much more flexibility. Where Sequential is a straight line, a Module model is a graph, where the nodes of the layers can connect in many more ways than one.\n",
    "\n",
    "In the visual example below, the one possible direction of the movement Sequential model is shown in contrast to a skip connection, which is just one of the many ways a Module model can be constructed. A skip connection, as you might have guessed, skips some layer in the network and feeds the output to a later layer in the network. Don't worry, you'll be spending more time with skip connections very soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufRKHn8yRU44"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/35547c07c53ba9c06a6fa5866ef4620471717820/C4%20-%20Convolutional%20Neural%20Networks/Week%201/images/seq_vs_func.png\" style=\"width:350px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nry0w7uRU44"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 - Load the SIGNS Dataset\n",
    "\n",
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFFC__3HRU44"
   },
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWpNIF5iRU45"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/35547c07c53ba9c06a6fa5866ef4620471717820/C4%20-%20Convolutional%20Neural%20Networks/Week%201/images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "6x3_nUolRU45",
    "outputId": "c4d9c4d6-db36-45f3-f54b-3a36685c0532"
   },
   "outputs": [],
   "source": [
    "# Example of an image from the dataset\n",
    "index = 9\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvZLAmBDRU45"
   },
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Split the Data into Train/Test Sets\n",
    "\n",
    "In Course 2, you built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPV2tZAeRU45"
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfldPwLpRU45",
    "outputId": "e110f805-3709-4631-96ef-5fa15589c391"
   },
   "outputs": [],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # Convert to (N, C, H, W)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "Y_train = torch.tensor(np.float32(Y_train))\n",
    "Y_test = torch.tensor(np.float32(Y_test))\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joQuQe5lRU49"
   },
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward Propagation\n",
    "\n",
    "In PyTorch, there are built-in functions that implement the convolution steps for you. In the [nn.Module API](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), you create a graph of layers.\n",
    "\n",
    "The following model could also be defined using the Sequential API, as in the previous part, since the information flow is on a single line. But don't deviate. What we want you to learn is to use the Module API. Module can be seen as a generalization of Sequential; Sequential does more for you but operates on the assumption that the network is strictly feed-forward. Module does less for you, but also assumes less about the network you are building, leaving more for you to define in forward propogation.\n",
    "\n",
    "Inside the model class definition you will need to define your various layers; unlike in Sequential, the order does not matter since we will be defining our own forward propogation funciton later. Note that these are example function and layer names and you should probably not choose these:\n",
    "```\n",
    "class arbitrary_model(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(convolutional_model, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.first_layer_function_name = nn.SomeKindaLayer(someParameter = value)\n",
    "        self.second_layer_name = nn.AnotherLayer()\n",
    "        self.third_layer_name = nn.DifferentLayer(parameter = value)\n",
    "```\n",
    "        \n",
    "\n",
    "After our layer functions have been defined in the `__init__` method, begin building your graph of layers by defining the forward propogation function for your model. Simply call your first layer function on your input:\n",
    "```\n",
    "def forward(self, x):\n",
    "   x = self.first_layer_function_name(x)\n",
    "```\n",
    "\n",
    "Then, create a new node in the graph of layers by calling a layer on the calling other layers on that output:\n",
    "```\n",
    "   x = self.second_layer_name(x)\n",
    "   x = self.third_layer_name(x)\n",
    "   #etc.\n",
    "   return x\n",
    "```\n",
    "\n",
    "- **nn.Conv2d(in_channels=input_shape[0], out_channels=8, kernel_size=(4, 4), stride=(1, 1), padding=(p, p)):** Read the full documentation on [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "\n",
    "- **nn.MaxPool2d(kernel_size=(f, f), stride=(s, s), padding=(p, p)):** `MaxPool2D()` downsamples your input using a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window.  For max pooling, you usually operate on a single example at a time and a single channel at a time. Read the full documentation on [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).\n",
    "\n",
    "- **nn.ReLU():** computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n",
    "\n",
    "- **nn.Flatten()**: given a tensor \"P\", this function takes each training (or test) example in the batch and flattens it into a 1D vector.  \n",
    "\n",
    "    * If a tensor P has the shape (batch_size,h,w,c), it returns a flattened tensor with shape (batch_size, k), where $k=h \\times w \\times c$.  \"k\" equals the product of all the dimension sizes other than the first dimension.\n",
    "    \n",
    "    * For example, given a tensor with dimensions [100, 2, 3, 4], it flattens the tensor to be of shape [100, 24], where 24 = 2 * 3 * 4.  You can read the full documentation on [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html).\n",
    "\n",
    "- **nn.Linear(in_features= , out_features=):** given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "In the last function above (`nn.Linear`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.\n",
    "\n",
    "Lastly, before creating the model, you'll need to define the output using the last of the function's compositions (in this example, a Linear layer):\n",
    "\n",
    "- **outputs = nn.Linear(in_features=64, out_features=6,)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtpXwAKYRU49"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - convolutional_model\n",
    "\n",
    "Implement the `convolutional_model` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> LINEAR`. Use the functions above!\n",
    "\n",
    "Also, plug in the following parameters for all the steps:\n",
    "\n",
    " - [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): Use 8 output channels, 4 by 4 kernel, stride 1, padding 1 by 1 (for in_channels, use input_shape[0] to get the original number of channels)\n",
    " - [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    " - [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html): Use an 8 by 8 kernel size and an 8 by 8 stride, padding 1 by 1\n",
    " - **Conv2d**: From 8 in channels, use 16 out channels of 2 by 2 kernels, stride 1, padding is 1 by 1\n",
    " - **ReLU**\n",
    " - **MaxPool2d**: Use a 4 by 4 kernel size and a 4 by 4 stride, padding 1 by 1\n",
    " - [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) the previous output.\n",
    " - Fully-connected ([Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) layer: Apply a fully connected layer with 6 neurons.\n",
    "\n",
    " <font color='red'> **rubric={30 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "IvIbFAQGRU4-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58643806aa8380c96225fc8b4c5e7aa",
     "grade": false,
     "grade_id": "cell-dac51744a9e03f51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convolutional_model\n",
    "\"\"\"\n",
    "    Implements __init__ for the model's layers.\n",
    "\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> LINEAR\n",
    "\n",
    "    Note that for simplicity and grading purposes, you'll hard-code some values\n",
    "    such as the stride and kernel sizes.\n",
    "    Normally, functions should take these values as function parameters.\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- a tuple of the input shape of the model (in this case, 3x64x64)\n",
    "\n",
    "    Returns:\n",
    "    model -- nn.Module model (object containing the information for the entire training process)\n",
    "\"\"\"\n",
    "class convolutional_model(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(convolutional_model, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Define the forward pass - just pass x through each layer in sequence. You could do more advanced things here, which is the advantage of Module over Sequential.\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WXoTH57iUWW",
    "outputId": "61dc71b5-ce66-4a03-a538-79243bd45d29"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# Example usage:\n",
    "input_shape = (3, 64, 64)  # Assuming input shape is (channels, width, height)\n",
    "sign_model = convolutional_model(input_shape).to(device)\n",
    "summary(sign_model, input_size = (batch_size, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **rubric={5 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_7ZOZVr1hen"
   },
   "outputs": [],
   "source": [
    "criterion =   # YOUR CODE HERE Use appropriate loss function \n",
    "optimizer = optim.Adam(sign_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoQ-XlSbRU4-"
   },
   "source": [
    "Your Module model should now be ready to use, just like the Sequential model. It took a little more prep, but now that it is complete, training the models is mostly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU8hlLODRU4-"
   },
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Smr8gfRU4-"
   },
   "source": [
    "Repeat the same steps as earlier to get the data ready for training: Convert to a Torch tensor, reshape the data into the [Samples, Channels, Width, Height] format, send it to a TensorDataset, and finally get it ready for batching as a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3-QP-enjG18"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe83-ujARU4-"
   },
   "source": [
    "Before we train, let's also set up some way to store the history of model loss after each step. We are going to record and graph the following for each epoch:\n",
    "- Training Loss\n",
    "- Validation Loss (using \"test\" set)\n",
    "- Training Accuracy\n",
    "- Validation Accuracy (using \"test\" set)\n",
    "Yes, this means that during every training step, we will be \"validating\" as well. We shouldn't call this \"testing\" since it is being used to gauge how the model is trained. This lets us see after how many epochs the model begins to overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyKD6XMJRU4_"
   },
   "outputs": [],
   "source": [
    "history = {'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **rubric={15 points}** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uz9oSBdRRU4_",
    "outputId": "922a4a8e-f465-45d9-8144-c420dd1cd7d0"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):  # Adjust num_epochs as needed\n",
    "    sign_model.train()                                          # Be sure to set the model to training mode to start!\n",
    "    counter = 0                                             # Counts the number of batches we have gone through - this is needed to calculate the training accuracy.\n",
    "    temp_acc = 0.0                                          # Stores the sum of all training accuracy readings for a given epoch.\n",
    "    temp_loss = 0.0                                         # Stores the sum of the loss found each each batch by the built-in loss function, whatever that was defined as.\n",
    "    for i, (inputs, labels) in enumerate(train_loader):         # Loop through the training DataLoader - this acts as our batcher.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        # YOUR CODE STARTS HERE (Hint: Forward pass, loss computation, backward pass, optimizer step) You may need to update the variable names in the following starter code here based on your implementation.\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)    # Get the class (index) with the highest logit for each sample.\n",
    "                                                # (print out the object \"outputs\" and read docs on torch.max if you're curious why this is written like this)\n",
    "        _, labels_num = torch.max(labels, 1)\n",
    "\n",
    "        correct = (predicted == labels_num).sum().item() # Get the number of correctly predicted items in the batch.\n",
    "\n",
    "        total = len(predicted)\n",
    "        train_accuracy = float(correct) / total     # Self-explanatory\n",
    "\n",
    "        temp_acc += train_accuracy\n",
    "        temp_loss += (loss.item())\n",
    "\n",
    "\n",
    "        counter += 1.0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {temp_loss / counter}\")\n",
    "    history['train_loss'].append(temp_loss / counter)\n",
    "    history['train_acc'].append(temp_acc / counter)\n",
    "\n",
    "    sign_model.eval()\n",
    "\n",
    "    accumulated_test_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    # Iterate over the test set\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = sign_model(inputs)\n",
    "        test_loss = criterion(outputs, labels).item()  # .item() to get scalar value\n",
    "        accumulated_test_loss += test_loss\n",
    "        _, predicted = torch.max(outputs[0].data, 0)    # These come out with an extra dimension due to being in batches of 1. The [0] eliminates that problem.\n",
    "        _, num_label = torch.max(labels[0].data, 0)\n",
    "        total += 1\n",
    "        if (predicted == num_label): correct += 1.0\n",
    "\n",
    "    # Calculate average test loss if accumulated_test_loss is not zero\n",
    "    avg_test_loss = accumulated_test_loss / len(test_loader)\n",
    "    history['val_loss'].append(avg_test_loss)\n",
    "\n",
    "    test_accuracy = float(correct) / total\n",
    "    history['val_acc'].append(test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAev3B7KRU4_"
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Training History\n",
    "\n",
    "PyTorch does not have a built-in \"history\" object like TensorFlow does, which is why we made one in the previous part. Now we can observe these values and start charting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQUdvz-KRU4_"
   },
   "source": [
    "Now visualize the loss over time by turning the measurements into charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zJPCCo3TRU4_",
    "outputId": "3cd5752c-5a93-47a9-a290-96c8840e8313"
   },
   "outputs": [],
   "source": [
    "# This code was written by the original author of the TensorFlow version of this worksheet. It works but I take no responsibility for the readability.\n",
    "# If you stored your data points differently than I did, this will probably need to be modified.\n",
    "df_loss_acc = pd.DataFrame(history)\n",
    "df_loss= df_loss_acc[['train_loss','val_loss']]\n",
    "df_loss.rename(columns={'train_loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['train_acc','val_acc']]\n",
    "df_acc.rename(columns={'train_acc':'train','val_acc':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSuUVoShRU4_"
   },
   "source": [
    "**Congratulations**! You've finished the assignment and built two models: One that recognizes  smiles, and another that recognizes SIGN language with almost 80% accuracy on the test set. In addition to that, you now also understand the applications of two Torc APIs: Sequential and Module. Nicely done!\n",
    "\n",
    "By now, you know a bit about how the Module API works and may have glimpsed the possibilities. In your next assignment, you'll really get a feel for its power when you get the opportunity to build a very deep ConvNet, using ResNets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xurL5kl_RU5A"
   },
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Bibliography\n",
    "\n",
    "You're always encouraged to read the official documentation. To that end, you can find the docs for the Sequential and Module APIs here:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
